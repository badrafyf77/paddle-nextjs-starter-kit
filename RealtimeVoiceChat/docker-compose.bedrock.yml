# Docker Compose for Voice Chat with Bedrock Agent
# This removes Ollama and uses AWS Bedrock for LLM

services:
  # Your FastAPI Application Service
  app:
    build: .
    image: realtime-voice-chat:latest
    container_name: realtime-voice-chat-app
    ports:
      - '8000:8000'
    environment:
      # --- Bedrock Configuration ---
      - LLM_PROVIDER=bedrock
      - BEDROCK_AGENT_ID=${BEDROCK_AGENT_ID:-VUEHUL2HDK}
      - BEDROCK_AGENT_ALIAS_ID=${BEDROCK_AGENT_ALIAS_ID:-JBU23UII65}
      - BEDROCK_REGION=${BEDROCK_REGION:-us-west-2}

      # --- AWS Credentials ---
      # Option 1: Use environment variables
      - AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID}
      - AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY}
      - AWS_SESSION_TOKEN=${AWS_SESSION_TOKEN} # Optional, for temporary credentials
      - AWS_REGION=${AWS_REGION:-us-west-2}

      # Option 2: Use AWS profile (mount ~/.aws)
      # See volumes section below

      # --- Other App Environment Variables ---
      - LOG_LEVEL=${LOG_LEVEL:-INFO}
      - MAX_AUDIO_QUEUE_SIZE=${MAX_AUDIO_QUEUE_SIZE:-50}
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
      - HF_HOME=/home/appuser/.cache/huggingface
      - TORCH_HOME=/home/appuser/.cache/torch
    volumes:
      # Optional: Mount code for live development
      # - ./code:/app/code

      # Mount cache directories
      - huggingface_cache:/home/appuser/.cache/huggingface
      - torch_cache:/home/appuser/.cache/torch

      # Optional: Mount AWS credentials (if using AWS CLI profile)
      # - ~/.aws:/home/appuser/.aws:ro

    # No depends_on needed (no Ollama service)

    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu, compute, utility]
    restart: unless-stopped

# Define named volumes for persistent data
volumes:
  huggingface_cache:
    driver: local
  torch_cache:
    driver: local
# Note: No ollama service or ollama_data volume needed!
