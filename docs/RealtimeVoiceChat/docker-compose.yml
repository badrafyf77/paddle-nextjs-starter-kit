services:
  # Your FastAPI Application Service
  app:
    build: .
    image: realtime-voice-chat:latest
    container_name: realtime-voice-chat-app
    ports:
      - '8080:8000' # Changed to 8080 since vLLM uses 8000
    environment:
      # --- LLM Configuration (vLLM) ---
      - LLM_PROVIDER=vllm
      - LLM_MODEL=Qwen/Qwen2.5-3B-Instruct
      - VLLM_BASE_URL=http://vllm:8000/v1

      # --- Other App Environment Variables ---
      - LOG_LEVEL=${LOG_LEVEL:-INFO}
      - MAX_AUDIO_QUEUE_SIZE=${MAX_AUDIO_QUEUE_SIZE:-50}
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
      - HF_HOME=/home/appuser/.cache/huggingface
      - TORCH_HOME=/home/appuser/.cache/torch
    volumes:
      # Mount cache directories
      - huggingface_cache:/home/appuser/.cache/huggingface
      - torch_cache:/home/appuser/.cache/torch
    depends_on:
      - vllm
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu, compute, utility]
    restart: unless-stopped

  # vLLM Server Service - Optimized for multiple concurrent users
  vllm:
    image: vllm/vllm-openai:latest
    container_name: realtime-voice-chat-vllm
    ports:
      - '8000:8000'
    command:
      - --model
      - Qwen/Qwen2.5-3B-Instruct
      - --dtype
      - auto
      - --max-model-len
      - '4096'
      - --gpu-memory-utilization
      - '0.9'
      # Multi-user optimizations
      - --max-num-seqs
      - '32' # Handle up to 32 concurrent requests
      - --enable-chunked-prefill
      - --max-num-batched-tokens
      - '8192'
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
      - HF_HOME=/root/.cache/huggingface
    volumes:
      - huggingface_cache:/root/.cache/huggingface
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu, compute, utility]
    restart: unless-stopped
    shm_size: '4gb' # Shared memory for better performance

# Define named volumes for persistent data
volumes:
  huggingface_cache:
    driver: local
  torch_cache:
    driver: local
