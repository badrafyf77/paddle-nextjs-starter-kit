# Voice Chat Configuration
# Copy this to .env and configure your settings

# ============================================
# OPTION 1: vLLM (High-Performance Multi-User) - DEFAULT
# ============================================
LLM_PROVIDER=vllm
LLM_MODEL=meta-llama/Llama-3.2-3B-Instruct
VLLM_BASE_URL=http://vllm:8000/v1  # Use 'vllm' for Docker, 'localhost' for local

# # ============================================
# # OPTION 2: Bedrock Agent (AWS Managed LLM)
# # ============================================
# LLM_PROVIDER=bedrock
# BEDROCK_AGENT_ID=YOUR_AGENT_ID
# BEDROCK_AGENT_ALIAS_ID=YOUR_ALIAS_ID
# BEDROCK_REGION=us-west-2

# # AWS Credentials (required for Bedrock)
# AWS_ACCESS_KEY_ID=your_access_key
# AWS_SECRET_ACCESS_KEY=your_secret_key
# # AWS_SESSION_TOKEN=your_session_token  # Optional, for temporary credentials
# AWS_REGION=us-west-2

# ============================================
# OPTION 3: Ollama (Local LLM - Single User)
# ============================================
# LLM_PROVIDER=ollama
# LLM_MODEL=llama3.2:latest
# OLLAMA_BASE_URL=http://ollama:11434  # Set in docker-compose.yml

# ============================================
# OPTION 4: OpenAI
# ============================================
# LLM_PROVIDER=openai
# LLM_MODEL=gpt-4
# OPENAI_API_KEY=your_openai_key

# ============================================
# OPTION 5: LM Studio (Local)
# ============================================
# LLM_PROVIDER=lmstudio
# LLM_MODEL=your-model-name
# LMSTUDIO_BASE_URL=http://localhost:1234/v1

# ============================================
# Common Settings (All Providers)
# ============================================
LOG_LEVEL=INFO
MAX_AUDIO_QUEUE_SIZE=50

# TTS Engine (unchanged regardless of LLM provider)
# TTS_ENGINE=kokoro  # Options: kokoro, orpheus, coqui
